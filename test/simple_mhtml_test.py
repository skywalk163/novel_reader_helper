#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„MHTMLæ–‡ä»¶å†…å®¹æå–æµ‹è¯•
ä¸“æ³¨äºç›´æ¥è§£æï¼Œé¿å…æµè§ˆå™¨æ¸²æŸ“çš„å¤æ‚æ€§
"""

import os
import re
from bs4 import BeautifulSoup
from urllib.parse import unquote

def analyze_mhtml_structure(file_path):
    """åˆ†æMHTMLæ–‡ä»¶ç»“æ„"""
    print("=== MHTMLæ–‡ä»¶ç»“æ„åˆ†æ ===")
    
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
            content = file.read()
        
        print(f"æ–‡ä»¶æ€»é•¿åº¦: {len(content)} å­—ç¬¦")
        
        # å¯»æ‰¾HTMLå†…å®¹
        html_matches = list(re.finditer(r'<html[\s\S]*?</html>', content, re.IGNORECASE))
        print(f"æ‰¾åˆ° {len(html_matches)} ä¸ªHTMLå—")
        
        # å¯»æ‰¾å¯èƒ½çš„å°è¯´å†…å®¹æ ‡è¯†
        novel_patterns = [
            r'class="read-content"',
            r'class="j_readContent"',
            r'class="chapter"',
            r'id="content"',
            r'<h1[^>]*>.*?ç« .*?</h1>',
            r'<div[^>]*content[^>]*>'
        ]
        
        for pattern in novel_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                print(f"æ‰¾åˆ°æ¨¡å¼ '{pattern}': {len(matches)} æ¬¡")
        
        return content
        
    except Exception as e:
        print(f"åˆ†ææ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def extract_from_mhtml_advanced(file_path):
    """é«˜çº§MHTMLå†…å®¹æå–"""
    print("\n=== é«˜çº§MHTMLå†…å®¹æå– ===")
    
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
            mhtml_content = file.read()
        
        # å¯»æ‰¾æ‰€æœ‰HTMLå—
        html_blocks = re.findall(r'<html[\s\S]*?</html>', mhtml_content, re.IGNORECASE)
        print(f"æ‰¾åˆ° {len(html_blocks)} ä¸ªHTMLå—")
        
        results = []
        
        for i, html_block in enumerate(html_blocks):
            print(f"\n--- å¤„ç†HTMLå— {i+1} ---")
            
            soup = BeautifulSoup(html_block, 'html.parser')
            
            # æŸ¥æ‰¾æ ‡é¢˜ - ä½¿ç”¨å¤šç§é€‰æ‹©å™¨
            title_selectors = [
                'h1.j_chapterName',
                'h1[class*="chapter"]',
                '.chapter-title',
                'h1',
                'h2',
                '.title'
            ]
            
            title = "æœªæ‰¾åˆ°æ ‡é¢˜"
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.get_text().strip():
                    title = title_elem.get_text().strip()
                    print(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ°æ ‡é¢˜")
                    break
            
            # æŸ¥æ‰¾å†…å®¹ - ä½¿ç”¨å¤šç§é€‰æ‹©å™¨
            content_selectors = [
                '.read-content',
                '.j_readContent',
                '[class*="read-content"]',
                '[class*="chapter-content"]',
                '[id*="content"]',
                '.content',
                'main',
                'article'
            ]
            
            content = "æœªæ‰¾åˆ°æ­£æ–‡å†…å®¹"
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # å°è¯•æå–æ‰€æœ‰æ®µè½
                    paragraphs = content_elem.find_all(['p', 'div'])
                    if paragraphs:
                        content = '\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
                    else:
                        content = content_elem.get_text().strip()
                    
                    if len(content) > 100:  # åªæ¥å—è¶³å¤Ÿé•¿çš„å†…å®¹
                        print(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ°å†…å®¹")
                        break
            
            # å¤„ç†å¯èƒ½çš„ç¼–ç é—®é¢˜
            if title and title != "æœªæ‰¾åˆ°æ ‡é¢˜":
                # å°è¯•URLè§£ç 
                try:
                    decoded_title = unquote(title)
                    if decoded_title != title:
                        title = decoded_title
                        print("æ ‡é¢˜ç»è¿‡URLè§£ç ")
                except:
                    pass
            
            result = {
                'block_index': i + 1,
                'title': title,
                'content': content,
                'content_length': len(content)
            }
            
            results.append(result)
            
            print(f"æ ‡é¢˜: {title}")
            print(f"å†…å®¹é•¿åº¦: {len(content)}")
            if len(content) > 50:
                print(f"å†…å®¹é¢„è§ˆ: {content[:200]}...")
        
        return results
        
    except Exception as e:
        print(f"æå–å†…å®¹æ—¶å‡ºé”™: {e}")
        return []

def test_current_browser_integration(results):
    """æµ‹è¯•ä¸å½“å‰æµè§ˆå™¨é›†æˆçš„å…¼å®¹æ€§"""
    print("\n=== æµè§ˆå™¨é›†æˆå…¼å®¹æ€§æµ‹è¯• ===")
    
    if not results:
        print("æ²¡æœ‰æå–ç»“æœå¯ä¾›æµ‹è¯•")
        return
    
    # æ‰¾åˆ°æœ€ä½³ç»“æœï¼ˆå†…å®¹æœ€é•¿çš„ï¼‰
    best_result = max(results, key=lambda x: x['content_length'])
    
    print(f"æœ€ä½³æå–ç»“æœæ¥è‡ªHTMLå— {best_result['block_index']}")
    print(f"æ ‡é¢˜: {best_result['title']}")
    print(f"å†…å®¹é•¿åº¦: {best_result['content_length']} å­—ç¬¦")
    
    if best_result['content_length'] > 100:
        print("âœ… æå–æˆåŠŸï¼è¿™ä¸ªç»“æœå¯ä»¥ç”¨äºæµè§ˆå™¨é›†æˆ")
        
        # æ¨¡æ‹Ÿæµè§ˆå™¨æå–ç»“æœæ ¼å¼
        browser_compatible_result = {
            'title': best_result['title'],
            'text': best_result['content'],
            'word_count': best_result['content_length'],
            'images': [],  # MHTMLæ–‡ä»¶ä¸­çš„å›¾ç‰‡éœ€è¦ç‰¹æ®Šå¤„ç†
            'chapter_info': {
                'source': 'MHTMLæ–‡ä»¶',
                'extraction_method': 'direct_parsing'
            }
        }
        
        print("æµè§ˆå™¨å…¼å®¹æ ¼å¼:")
        print(f"  æ ‡é¢˜: {browser_compatible_result['title']}")
        print(f"  å­—æ•°: {browser_compatible_result['word_count']}")
        print(f"  å†…å®¹é¢„è§ˆ: {browser_compatible_result['text'][:100]}...")
        
        return browser_compatible_result
    else:
        print("âŒ æå–å¤±è´¥ï¼Œå†…å®¹å¤ªçŸ­æˆ–ä¸ºç©º")
        return None

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    file_path = r"E:\360Downloads\test.mhtml"
    
    print("ğŸ¯ ç®€åŒ–MHTMLæ–‡ä»¶å†…å®¹æå–æµ‹è¯•")
    print(f"ç›®æ ‡æ–‡ä»¶: {file_path}")
    
    if not os.path.exists(file_path):
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        return
    
    # 1. åˆ†ææ–‡ä»¶ç»“æ„
    mhtml_content = analyze_mhtml_structure(file_path)
    if not mhtml_content:
        return
    
    # 2. é«˜çº§å†…å®¹æå–
    results = extract_from_mhtml_advanced(file_path)
    
    # 3. æµè§ˆå™¨é›†æˆæµ‹è¯•
    compatible_result = test_current_browser_integration(results)
    
    # 4. æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    if compatible_result:
        print("âœ… æµ‹è¯•æˆåŠŸï¼")
        print("å¯ä»¥å°†è¿™ç§æå–æ–¹æ³•é›†æˆåˆ°ç°æœ‰çš„å°è¯´é˜…è¯»å™¨ä¸­")
        print("\nå»ºè®®çš„é›†æˆæ­¥éª¤:")
        print("1. åœ¨browser.pyä¸­æ·»åŠ MHTMLæ–‡ä»¶æ£€æµ‹")
        print("2. å½“æ£€æµ‹åˆ°æœ¬åœ°MHTMLæ–‡ä»¶æ—¶ï¼Œä½¿ç”¨ç›´æ¥è§£ææ–¹æ³•")
        print("3. è·³è¿‡JavaScriptæ‰§è¡Œï¼Œç›´æ¥è¿”å›è§£æç»“æœ")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥")
        print("éœ€è¦è¿›ä¸€æ­¥åˆ†æMHTMLæ–‡ä»¶æ ¼å¼æˆ–å°è¯•å…¶ä»–æå–æ–¹æ³•")

if __name__ == "__main__":
    main()